# Notes-RL
Collection of paper notes (PDF+LaTeX) in reinforcement learning, with compact summary and detailed mathematical derivations.

# Textbook
- [Sutton, Introduction to Reinforcement Learning, 2nd edition](/Sutton%20-%20Introduction%20to%20RL)

# Derivative-Free Optimization
- Szita et al., Learning Teris using the Noisy Cross-Entropy Method

# RL
- Nachum et al., Bridging the Gap Between Valud and Policy Based Reinforcement Learning

# Policy Gradients
- Williams, Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning
- Silver et al., A3C 
  -- Only focus on A2C version. Asynchronuous not very important
- Precup et al., DRL that Matters
- Schulman, High-dimensional continuous control using generalized advantage estimation
- Schulman, Benchmarking deep reinforcement learning for continuous control
- Kakade, Approximately Optimal Approximate Reinforcement Learning
- Silver et al., DPG
- Silver et al., DDPG
- Schulman et al., TRPO
- Schulman et al., PPO
- Liu et al., Stein Variational Policy Gradient
- Gruslys et al., The Reactor
- Wang et al., Sample Efficient Actor-Critic with Experience Reply
- Gu et al., Q-Prop
- Gu et al., Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning

# DQN
- Silver et al., Human-level control through deep reinforcement learning
- Silver et al. Double Q-Learning
- Silver et al. Dueling network architectures for deep reinforcement learning
- Silver et al. Prioritized experience replay
- Silver et al., Rainbow: Combining Improvements in DRL
- Bellemare et al., A Distributional Perspective on Reinforcement Learning

# Model-based RL & Planning
- Doll et al., The ubiquity of model-based reinforcement learning
- Tamar et al., Value Iteration Networks
- Karkus et al., QMDP-Net: Deep Learning for Planning under Partial Observability
- Tamar et al., Learning Generalized Reactive Policies using Deep Neural Networks
- Singh et al., Value Prediction Networks
- Salakhutdinov et al., LSTM Iteration Networks: An Exploration of Differentialble Path Finding
- Gal et al., Improving PILCO with Bayesian Neural Network Dynamics Models
- Meger et al., Synthesizing Neural Network Controllers with Probabilistic Model-based Reinforcement Learning
- Wierstra et al., Learning model-based planning from scratch
- Gu et al., Continuous Deep Q-Learning with Model-based Acceleration
- Lecun et al., Model-Based Planning in Discrete and Continuous Actions
- Silver et al., The Predictron: End-To-End Learning and Planning
- Weber et al., Imagination-Augmented Agents for Deep Reinforcement Learning
- Li et al., Iterative Linear Quadratic Regulator Design for Nonlinear Biological Movement Systems
- Chockalingam et al., Differentiable Neural Planners with Temporally Extended Actions
- Mishra et al., Prediction and Control with Temporal Segment Models
- Metz et al., Discrete Sequential Prediction of Continuous Actions for Deep RL
- Moerland et al., Learning Multimodal Transition Dynamics for Model-Based Reinforcement Learning
- Chiappa et al., Recurrent Environment Simulators
- Anthony et al., Thinking Fast and Slow with Deep Learning and Tree Search
- Graves et al., Strategic Attentive Writer for Learning Macro-Actions
- Sukhbaatar et al., Composable Planning with Attributes
- Vinyals et al., Metacontrol for adaptive imagination-based optimization
- Gerstner et al., Efficient Model-based Deep Reinforcement Learning with Variational State Tabulation
- Whiteson et al., TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning
- Abbeel et al., Universal Planning Networks
- Dinh et al., Learning Awareness Models
- Abbeel et al., Model-ensemble Trust-Region Policy Optimization
- Levine et al., Model-based Value Expansion for Efficient Model-Free Reinforcement Learning
- Levine et al., Recall Traces: Backtracking Models for Efficient Reinforcement Learning
- Levine et al., Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning
- Levine et al., Temporal Difference Models: Model-Free Deep RL for Model-Based Control

# Exploration in RL
- Ostand et al. Deep Exploration via Bootstrapped DQN
- Osband et al. (More) efficient reinforcement learning via posterior sampling
- Houthooft et al., VIME
- Ostrovski et al., Count-Based Exploration with Neural Density Models
- Tang et al., #Exploration: A Study of Count-based Exploration for Deep Reinforcement Learning
- Fortunato et al., Noisy Networks for Exploration
- Plappert et al., Parameter Space Noise for Exploration
- Wierstra et al., Learning and Querying Fast Generative Models for Reinforcement Learning

# Meta-RL

# Curriculum (Lifelong) RL
- [Schmidhuber, PowerPlay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem](/PowerPlay)

# Bayesian RL
- Tamar et al., Bayesian Reinforcement Learning: A Survey

# Neuroscience
- Botvinick et al., The hippocampus as a predictive map, Nature
- Hassabis et al., Neuroscience-Inspired Artificial Intelligence

# Deep learning
- Dumoulin, A guide to convolution arithmetic for deep learning
- Bottou, Stochastic Gradient Descent Tricks
- Kingma et al., Auto-Encoding Variational Bayes

# Bayesian Neural Networks
- Blundell et al., Weight Uncertainty in Neural Networks
- Blundell et al., Bayesian Recurrent Neural Networks
- Gal et al., What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision ?
- Hernandez-Lobato et al., Black-Box alpha-Divergence Minimization
- Blei et al., Variational Inference: A Review for Statisticians
- Roeder et al., Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference
- Lakshminarayanan et al., Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
