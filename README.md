# Paper-Notes-RL
Collection of paper notes (PDF+Latex) in reinforcement learning, with compact summary and detailed mathematical derivations.

# Derivative-Free Optimization
- Szita et al., Learning Teris using the Noisy Cross-Entropy Method

# Policy Gradients
- Williams, Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning
- Silver et al., A3C 
  -- Only focus on A2C version. Asynchronuous not very important
- Precup et al., DRL that Matters
- Gu et al. Continuous deep Q-learning with model-based acceleration
- Schulman, High-dimensional continuous control using generalized advantage estimation
- Schulman, Benchmarking deep reinforcement learning for continuous control
- Kakade, Approximately Optimal Approximate Reinforcement Learning
- Silver et al., DPG
- Silver et al., DDPG

# DQN
- Silver et al. Double Q-Learning
- Silver et al. Dueling network architectures for deep reinforcement learning
- Silver et al. Prioritized experience replay
- Silver et al., Rainbow: Combining Improvements in DRL

# Exploration
- Ostand et al. Deep Exploration via Bootstrapped DQN
- Osband et al. (More) efficient reinforcement learning via posterior sampling

# Neuroscience
- Botvinick et al., The hippocampus as a predictive map, Nature

# Deep learning
- Dumoulin, A guide to convolution arithmetic for deep learning
